/**
 * SIMD String Utilities - Optimized string operations using SSE4.2
 *
 * Provides SIMD-accelerated versions of common string operations:
 * - Fast strlen using PCMPISTRI instruction (SSE4.2)
 * - Optimized memcpy (compiler auto-vectorization)
 *
 * Automatically falls back to standard library functions when
 * SSE4.2 is not available.
 */

#pragma once

#include <cstddef>
#include <cstdint>
#include <cstring>

#ifdef __SSE4_2__
#include <nmmintrin.h>  // SSE4.2 intrinsics

namespace tpch {
namespace simd {

/**
 * Fast strlen using SSE4.2 PCMPISTRI instruction
 *
 * Processes 16 bytes at a time using SIMD comparison,
 * significantly faster than scalar byte-by-byte comparison.
 *
 * Performance: ~2-3Ã— faster than standard strlen for strings >16 bytes
 *
 * @param str Null-terminated string
 * @return Length of string (excluding null terminator)
 */
inline size_t strlen_sse42(const char* str) {
    const __m128i zero = _mm_setzero_si128();
    const char* ptr = str;

    // Handle unaligned prefix (up to 15 bytes)
    // Process byte-by-byte until aligned to 16-byte boundary
    while ((reinterpret_cast<uintptr_t>(ptr) & 15) != 0) {
        if (*ptr == '\0') {
            return ptr - str;
        }
        ptr++;
    }

    // Process 16 bytes at a time using SSE4.2
    while (true) {
        // Load 16 bytes (aligned)
        __m128i chunk = _mm_load_si128(reinterpret_cast<const __m128i*>(ptr));

        // Compare each byte with zero
        // Returns index of first null byte, or 16 if not found
        int index = _mm_cmpistri(
            zero,                               // needle (null byte)
            chunk,                              // haystack (16 bytes)
            _SIDD_UBYTE_OPS |                  // operate on unsigned bytes
            _SIDD_CMP_EQUAL_EACH |             // compare for equality
            _SIDD_LEAST_SIGNIFICANT            // return least significant index
        );

        if (index != 16) {
            // Found null terminator at position 'index'
            return (ptr - str) + index;
        }

        ptr += 16;
    }
}

/**
 * Fast strlen with unaligned load support
 *
 * For cases where string alignment is unknown, this variant
 * uses unaligned loads (slightly slower but always safe).
 *
 * @param str Null-terminated string (may be unaligned)
 * @return Length of string
 */
inline size_t strlen_sse42_unaligned(const char* str) {
    const __m128i zero = _mm_setzero_si128();
    const char* ptr = str;

    while (true) {
        // Load 16 bytes (unaligned - safe but slower)
        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(ptr));

        int index = _mm_cmpistri(
            zero, chunk,
            _SIDD_UBYTE_OPS | _SIDD_CMP_EQUAL_EACH | _SIDD_LEAST_SIGNIFICANT
        );

        if (index != 16) {
            return (ptr - str) + index;
        }

        ptr += 16;
    }
}

/**
 * Batch string copy - leverages compiler auto-vectorization
 *
 * With -O3 -march=native, compiler will auto-vectorize this to AVX/SSE
 *
 * @param dest Destination buffer
 * @param src Source buffer
 * @param n Number of bytes to copy
 */
inline void memcpy_batch(void* dest, const void* src, size_t n) {
    // Compiler auto-vectorizes this with -ftree-vectorize
    std::memcpy(dest, src, n);
}

} // namespace simd
} // namespace tpch

#else  // No SSE4.2 support - fallback to standard library

namespace tpch {
namespace simd {

// Fallback implementations using standard library
inline size_t strlen_sse42(const char* str) {
    return std::strlen(str);
}

inline size_t strlen_sse42_unaligned(const char* str) {
    return std::strlen(str);
}

inline void memcpy_batch(void* dest, const void* src, size_t n) {
    std::memcpy(dest, src, n);
}

} // namespace simd
} // namespace tpch

#endif  // __SSE4_2__
